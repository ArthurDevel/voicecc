# Claude Code Voice Plugin

## High Level Goal

Build a Claude Code plugin that adds a `/voice` command. When triggered, it starts a local voice loop: the user speaks into their mic, speech gets transcribed locally, piped into the running Claude Code session (via the Agent SDK), and Claude Code's response gets spoken back via local TTS. Fully local STT/TTS/VAD pipeline -- no external API keys required for those. The Anthropic API key (already present for Claude Code) is reused for the Haiku endpointing fallback.

The voice loop stages:
1. Mic capture (node-record-lpcm16 / sox)
2. VAD detection (avr-vad / Silero VAD v5)
3. Local STT (sherpa-onnx / Whisper ONNX)
4. Semantic endpointing (VAD fast path + Haiku fallback for ambiguous cases)
5. Pipe transcription to Claude Code (Agent SDK V1, persistent session, streaming)
6. Narration of Claude's response via local TTS (kokoro-js)
7. Speaker playback with interruption support

Target latency: under 2 seconds from end-of-speech to first audio playback (excluding LLM inference time).

---

## File Tree

```
claude-code-voice/
├── .claude-plugin/
│   └── plugin.json                 # (NEW) Plugin manifest
├── skills/
│   └── voice/
│       └── SKILL.md                # (NEW) /voice skill -- instructs Claude to launch sidecar via Bash
├── sidecar/
│   ├── index.ts                    # (NEW) Entry point -- starts/stops the voice loop, signal handlers
│   ├── audio-capture.ts            # (NEW) Mic input via node-record-lpcm16 (raw PCM, endOnSilence=false)
│   ├── vad.ts                      # (NEW) Voice activity detection via avr-vad (accepts Float32Array)
│   ├── stt.ts                      # (NEW) Local streaming STT via sherpa-onnx (accepts Float32Array)
│   ├── tts.ts                      # (NEW) Local TTS via kokoro-js + speaker playback (24kHz output)
│   ├── endpointing.ts              # (NEW) Turn detection: VAD silence-based + optional Haiku semantic check
│   ├── claude-session.ts           # (NEW) Claude Agent SDK V1 session management + event mapping
│   ├── narration.ts                # (NEW) Streams Claude output into TTS-friendly text
│   └── types.ts                    # (NEW) Shared DTOs and interfaces
├── package.json                    # (NEW) Dependencies and scripts
├── tsconfig.json                   # (NEW) TypeScript config
└── README.md                       # (NEW) Setup instructions (sox, model download)
```

---

## Per-File Methods and Types

### types.ts (NEW)

Shared interfaces used across the sidecar modules.

#### Types

```
VoiceLoopConfig {
  sttModelPath: string                // path to sherpa-onnx Whisper ONNX model
  ttsVoice: string                    // kokoro voice ID
  ttsModelVariant: string             // default: "q8"
  modelCacheDir: string               // default: "~/.claude-voice-models/"
  sampleRate: number                  // default: 16000
  endpointing: EndpointingConfig
  narration: NarrationConfig
  claudeSession: ClaudeSessionConfig
  stopPhrase: string                  // default: "stop listening"
}

AudioFrame {
  pcm: Float32Array                   // normalized -1.0 to 1.0
  sampleRate: number
  timestamp: number
}

VadEvent {
  type: "SPEECH_START" | "SPEECH_CONTINUE" | "SPEECH_END" | "SILENCE"
  probability: number
  timestamp: number
}

TranscriptionResult {
  text: string
  isFinal: boolean
  timestamp: number
}

EndpointDecision {
  isComplete: boolean
  transcript: string
  method: "vad_fast" | "haiku_semantic" | "timeout"
}

ClaudeStreamEvent {
  type: "text_delta" | "tool_start" | "tool_end" | "result" | "error"
  content: string
  toolName?: string
}

VoiceLoopState {
  status: "idle" | "listening" | "processing" | "speaking"
  sessionId: string | null
}
```

---

### index.ts (NEW)

Entry point. Starts the voice loop, wires all modules together, handles shutdown and signal handlers (SIGINT, SIGTERM).

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `startVoiceLoop` | `config: VoiceLoopConfig` | `Promise<void>` | Initializes all modules (audio, VAD, STT, TTS, Claude session), registers signal handlers for cleanup, prints "Voice mode active" to stdout, and enters the main loop. Runs until `stopVoiceLoop` is called or stop phrase is detected. |
| `stopVoiceLoop` | none | `Promise<void>` | Gracefully shuts down: stops mic capture, frees VAD/STT/TTS resources, closes Claude session, prints "Voice mode stopped" to stdout. |
| `handleStateTransition` | `from: VoiceLoopState, event: string` | `VoiceLoopState` | Pure function that computes the next state. Prints state changes to stdout (e.g. "Listening...", "Processing..."). |

---

### audio-capture.ts (NEW)

Wraps node-record-lpcm16 to provide a stream of raw PCM audio chunks from the microphone.

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `startCapture` | `sampleRate: number` | `Readable` (Node stream of `Buffer`) | Starts sox-based mic recording with options: `sampleRate`, `channels: 1`, `audioType: 'raw'`, `endOnSilence: false`. Returns a readable stream of raw 16-bit signed PCM. Throws if sox is not installed. |
| `stopCapture` | none | `void` | Stops the recording process (kills sox child process). |
| `isCapturing` | none | `boolean` | Returns whether the mic is currently active. |
| `bufferToFloat32` | `buffer: Buffer` | `Float32Array` | Converts a raw 16-bit signed PCM Buffer to normalized Float32Array (-1.0 to 1.0). Used by VAD and STT which both expect Float32Array input. |

---

### vad.ts (NEW)

Wraps avr-vad (Silero VAD v5) to detect speech segments in the audio stream.

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `createVad` | none | `Promise<VadProcessor>` | Initializes Silero VAD v5 model via avr-vad's `RealTimeVAD.new()`. |
| `VadProcessor.processFrame` | `frame: Float32Array` | `Promise<VadEvent>` | Feeds a 1536-sample frame (16kHz) to the model. Async because avr-vad's processFrame returns a Promise. Maps avr-vad's `{probability, msg}` response to our `VadEvent` type. |
| `VadProcessor.reset` | none | `void` | Resets internal state (call between utterances). |
| `VadProcessor.destroy` | none | `void` | Frees ONNX resources. |

---

### stt.ts (NEW)

Local speech-to-text using sherpa-onnx with a Whisper ONNX model. Important: Whisper models in sherpa-onnx are **offline-only** (batch, not streaming). They use `createOfflineRecognizer`, not the online/streaming recognizer.

Architecture: VAD-segmented batch transcription. Audio is accumulated in a buffer during speech (between SPEECH_START and SPEECH_END). Once VAD signals SPEECH_END, the complete audio buffer is passed to the offline recognizer for batch transcription. This adds latency proportional to utterance length but is architecturally correct for Whisper.

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `createStt` | `modelPath: string` | `Promise<SttProcessor>` | Loads the sherpa-onnx offline recognizer (`createOfflineRecognizer`) with the specified Whisper ONNX model. Throws if model files are missing. |
| `SttProcessor.accumulate` | `samples: Float32Array` | `void` | Appends audio samples to the internal buffer. Call continuously during speech (between SPEECH_START and SPEECH_END). |
| `SttProcessor.transcribe` | none | `Promise<TranscriptionResult>` | Batch-transcribes the accumulated audio buffer using the offline recognizer. Creates an offline stream, feeds accumulated audio, decodes, and returns the result. Clears the buffer afterward. |
| `SttProcessor.clearBuffer` | none | `void` | Clears the accumulated audio buffer without transcribing (e.g. on interruption). |
| `SttProcessor.destroy` | none | `void` | Frees recognizer resources. |

---

### endpointing.ts (NEW)

Determines when the user is done speaking. Two-tier: fast VAD-based check (primarily silence duration), with optional Haiku semantic check for ambiguous cases.

#### Types

```
EndpointingConfig {
  silenceThresholdMs: number          // default: 700
  maxSilenceBeforeTimeoutMs: number   // default: 1200
  minWordCountForFastPath: number     // default: 2
  enableHaikuFallback: boolean        // default: true
}
```

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `createEndpointer` | `config: EndpointingConfig` | `Endpointer` | Creates an endpointer with the given thresholds. |
| `Endpointer.onVadEvent` | `event: VadEvent, currentTranscript: string` | `Promise<EndpointDecision>` | Processes a VAD event against the current transcript. Fast path: SPEECH_END + silence > `silenceThresholdMs` + word count >= `minWordCountForFastPath`. Does NOT rely on punctuation (local STT often omits it). Slow path: if silence > threshold but transcript is only 1 word, call Haiku for yes/no turn-completion check (piggybacks on existing Anthropic API key). Timeout path: silence > `maxSilenceBeforeTimeoutMs` = always complete. |
| `Endpointer.reset` | none | `void` | Resets timers and state for a new turn. |

---

### claude-session.ts (NEW)

Manages the Claude Code Agent SDK V1 session. Keeps a persistent session alive across voice turns. Contains a mapping layer that transforms `SDKMessage` (from the Agent SDK) into the simpler `ClaudeStreamEvent` used downstream.

Internally: creates an `AsyncGenerator` that yields `SDKUserMessage` objects. The `query()` function receives this generator as the `prompt` parameter with `includePartialMessages: true`. The `query()` call also sets `permissionMode: "bypassPermissions"` (required -- without this, the voice loop hangs on tool approval prompts) and a `systemPrompt` for voice-mode behavior.

The response stream of `SDKMessage` is mapped by checking `message.type`:
- `message.type === "stream_event"` -> check `message.event.type`:
  - `message.event.type === "content_block_start"` + `message.event.content_block.type === "tool_use"` -> `ClaudeStreamEvent { type: "tool_start", toolName: message.event.content_block.name }`
  - `message.event.type === "content_block_delta"` + `message.event.delta.type === "text_delta"` -> `ClaudeStreamEvent { type: "text_delta", content: message.event.delta.text }`
  - `message.event.type === "content_block_stop"` (if previous block was tool_use) -> `ClaudeStreamEvent { type: "tool_end" }`
- `message.type === "result"` -> `ClaudeStreamEvent { type: "result" }` or `{ type: "error" }` based on `message.is_error`

#### Types

```
ClaudeSessionConfig {
  maxTurns: number                    // default: 50
  allowedTools: string[]              // default: all
  permissionMode: string              // default: "bypassPermissions" (voice loop cannot prompt for approval)
  systemPrompt: string                // voice-mode instructions: "Respond concisely. You are in voice mode -- your responses will be spoken aloud. Keep answers conversational and brief."
}
```

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `createClaudeSession` | `config: ClaudeSessionConfig` | `Promise<ClaudeSession>` | Creates an Agent SDK V1 session with an AsyncGenerator prompt, `includePartialMessages: true`, `permissionMode`, and `systemPrompt`. Stores the `Query` object for interruption support. |
| `ClaudeSession.sendMessage` | `text: string` | `AsyncIterable<ClaudeStreamEvent>` | Yields an `SDKUserMessage` into the session's AsyncGenerator. Returns the mapped stream of `ClaudeStreamEvent` objects. |
| `ClaudeSession.interrupt` | none | `void` | Calls `query.interrupt()` on the SDK Query object to cancel the current in-flight agent turn. Prevents Claude from continuing to generate tokens after the user interrupts. |
| `ClaudeSession.close` | none | `Promise<void>` | Closes the session and cleans up the generator. |

---

### narration.ts (NEW)

Processes Claude's streaming output into TTS-friendly text. Two modes: response narration (speak actual text) and long-task summarization (periodic template-based summaries during tool use). Only emits text that should be spoken -- no skip/filter mechanism needed.

#### Types

```
NarrationConfig {
  summaryIntervalMs: number           // default: 12000 (12 seconds)
}
```

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `createNarrator` | `config: NarrationConfig` | `Narrator` | Creates the narration processor. |
| `Narrator.processEvent` | `event: ClaudeStreamEvent` | `string[]` | Processes a streaming event. For `text_delta`: accumulates text, emits at sentence boundaries. For `tool_start`: enters long_task mode, starts timer, emits "Running {toolName}...". In long_task mode: every `summaryIntervalMs`, emits "Still working on {lastToolName}...". For `tool_end`: clears current tool. For `result`/`error`: flushes remaining text. Returns array of strings to speak (often empty, sometimes 1-2 sentences). |
| `Narrator.flush` | none | `string[]` | Flushes any remaining buffered text. |
| `Narrator.reset` | none | `void` | Resets state for a new turn. |

---

### tts.ts (NEW)

Local text-to-speech via kokoro-js with speaker playback. kokoro-js outputs Float32Array at 24kHz. This module handles conversion to 16-bit PCM for the `speaker` npm package. Uses kokoro-js's built-in `TextSplitterStream` for efficient incremental text-to-speech. Handles interruption by destroying the current Speaker instance and clearing the audio queue.

#### Types

```
TtsConfig {
  voice: string                       // default: kokoro voice ID
  modelVariant: string                // default: "q8" (86MB quantized)
}
```

#### Methods

| Method | Arguments | Returns | Description |
|--------|-----------|---------|-------------|
| `createTts` | `config: TtsConfig` | `Promise<TtsPlayer>` | Initializes kokoro-js with `KokoroTTS.from_pretrained()` using the specified variant. Creates a Speaker instance configured for 24kHz mono 16-bit signed PCM (`{channels: 1, bitDepth: 16, sampleRate: 24000, signed: true}`). Throws if model download fails or model files are missing. |
| `TtsPlayer.speak` | `text: string` | `Promise<void>` | Converts text to audio via `tts.generate(text, {voice})`, converts the resulting Float32Array (24kHz) to 16-bit signed PCM Int16Array, writes to the Speaker instance. Resolves when playback completes. |
| `TtsPlayer.speakStream` | `texts: AsyncIterable<string>` | `Promise<void>` | Feeds text chunks into kokoro-js's `TextSplitterStream`, consumes the resulting audio chunks, converts each to 16-bit PCM, and writes to speaker sequentially. First audio plays while later chunks are still generating. |
| `TtsPlayer.interrupt` | none | `void` | Destroys the current `Speaker` instance, clears the pending audio buffer queue, creates a fresh Speaker for the next playback. |
| `TtsPlayer.isSpeaking` | none | `boolean` | Returns whether audio is currently playing. |
| `TtsPlayer.destroy` | none | `void` | Frees kokoro-js model resources. |

---

### skills/voice/SKILL.md (NEW)

The skill instructs Claude to launch the sidecar as a Bash command.

```yaml
---
name: voice
description: Start voice mode for hands-free interaction via microphone
allowed-tools: ["Bash"]
---
```

The skill body instructs Claude to run `npx tsx <plugin-root>/sidecar/index.ts` via the Bash tool. The sidecar runs as a foreground process and blocks until the user says the stop phrase or presses Ctrl+C.

---

## Important Technical Notes

### Voice Loop State Machine

The main loop in `index.ts` is a state machine with four states:

```
IDLE --> LISTENING --> PROCESSING --> SPEAKING --> LISTENING
                          |               |
                          v               v
                    (empty/error)   (user interrupts)
                          |               |
                          v               v
                       LISTENING       LISTENING
```

- **IDLE**: Initial state. Entered on start, transitions to LISTENING after init.
- **LISTENING**: Mic is active, VAD is processing frames. Audio is being fed to STT during speech. On SPEECH_END + endpoint decision "complete", check for stop phrase in transcript. If stop phrase detected, call `stopVoiceLoop()`. Otherwise transition to PROCESSING.
- **PROCESSING**: Transcription is sent to Claude session. Stream events flow through narration into TTS. Transitions to SPEAKING as soon as first TTS chunk starts playing. If Claude returns an error, empty response, or only tool calls with no narrable text, transition back to LISTENING directly (skip SPEAKING).
- **SPEAKING**: TTS is playing. Mic remains active for interruption detection. If VAD detects SPEECH_START sustained for >300ms (to prevent false triggers from ambient noise), call `ttsPlayer.interrupt()` AND `claudeSession.interrupt()` (cancels in-flight API generation) and transition back to LISTENING.

### Audio Pipeline Data Flow

```
Mic (sox, 16kHz 16-bit mono PCM Buffer)
  |
  └──> bufferToFloat32() --> Float32Array
         |
         ├──> VAD (1536-sample frames, ~96ms each)
         |      |
         |      ├── SPEECH_START --> start accumulating audio in STT buffer
         |      ├── SPEECH_CONTINUE --> keep accumulating audio in STT buffer
         |      └── SPEECH_END --> batch-transcribe accumulated buffer
         |
         └──> STT (accumulates during speech, batch-transcribes on SPEECH_END)
                |
                └── transcribe() --> endpointing decision
                                       |
                                       └── transcript --> Claude session
                                                            |
                                                            └── SDKMessage --> check message.type
                                                                  |            + message.event.type
                                                                  |            + message.event.delta.type
                                                                  |
                                                                  └── map to ClaudeStreamEvent
                                                                        |
                                                                        └── narration --> text chunks
                                                                                            |
                                                                                            └── kokoro TextSplitterStream --> audio (24kHz Float32)
                                                                                                                                |
                                                                                                                                └── float32ToInt16Pcm() --> Speaker (24kHz mono 16-bit signed)
```

### Sample Format Conversion

Two conversion points in the pipeline:
1. **Mic output -> VAD/STT input**: `bufferToFloat32()` in `audio-capture.ts` converts raw 16-bit signed PCM Buffer to Float32Array normalized -1.0 to 1.0. Formula: `sample / 32768.0`.
2. **TTS output -> Speaker input**: kokoro-js outputs Float32Array at 24kHz. Convert to 16-bit signed PCM for the `speaker` package. Formula: `Math.max(-1, Math.min(1, sample)) * 32767`. Speaker must be configured at 24kHz (not 16kHz).

### Endpointing Logic Detail

The two-tier approach avoids cutting users off mid-thought. Local STT often omits punctuation, so the fast path is primarily silence-based, not punctuation-based.

1. **Fast path** (0ms added latency): VAD reports SPEECH_END + silence exceeds `silenceThresholdMs` (700ms) + transcript word count >= `minWordCountForFastPath` (2). Return immediately.
2. **Slow path** (~200ms added latency): Silence exceeds threshold but transcript is very short (1 word). Fire a Haiku API call: "Is this a complete user turn? Respond yes or no." This piggybacks on the Anthropic API key already present for Claude Code. Uses `@anthropic-ai/sdk` directly (not the Agent SDK).
3. **Timeout path**: If silence exceeds `maxSilenceBeforeTimeoutMs` (1200ms) regardless of transcript state, treat as complete. User has clearly stopped talking.

### Narration Mode Switching

The narrator tracks tool activity to switch between modes:

- On `tool_start`: enter `long_task` mode. Record `toolName`. Emit "Running {toolName}...". Start a timer.
- While in `long_task` mode: every `summaryIntervalMs`, emit a simple template string "Still working on {lastToolName}..." using the most recent tool name. No LLM call needed -- keep it simple.
- On receiving `text_delta` events (Claude's actual text response): switch to `response` mode. Accumulate text, emit at sentence boundaries (split on `.!?` followed by space or end-of-string).
- On `result` or `error`: flush remaining text, reset.

### Speaker Playback and Interruption

kokoro-js generates Float32Array audio at 24kHz. The `speaker` npm package writes PCM to the system audio output. The interruption flow:

1. Mic stays active even during SPEAKING state.
2. VAD continues processing mic frames during playback.
3. On SPEECH_START during SPEAKING, require >300ms of sustained speech before triggering interruption (prevents false triggers from ambient noise or speaker echo).
4. On confirmed interruption: call `ttsPlayer.interrupt()` (destroys current Speaker, clears audio queue, creates fresh Speaker) AND `claudeSession.interrupt()` (cancels in-flight API generation to stop wasting tokens).
5. Transition back to LISTENING.

Note: echo cancellation is deferred. For v1, headphones are recommended. The 300ms sustained-speech filter provides basic protection against false triggers.

### Process Lifecycle and Cleanup

The sidecar registers `process.on('SIGINT')` and `process.on('SIGTERM')` handlers that call `stopVoiceLoop()`. This ensures:
- sox child process is killed (mic stops)
- ONNX resources are freed (VAD, STT)
- kokoro-js model resources are freed
- Speaker stream is destroyed
- Claude session is closed

Without signal handlers, the sox process would leak and the microphone would remain locked.

### Model Downloads

sherpa-onnx requires a Whisper ONNX model downloaded to disk. kokoro-js downloads its model from HuggingFace on first use (requires internet for first run).

On `startVoiceLoop`:
- Check sherpa-onnx model exists at `config.sttModelPath`. If missing, throw with a clear message including download URL and expected path.
- kokoro-js handles its own model loading. If it fails (offline, firewall), the error from `KokoroTTS.from_pretrained()` is caught and rethrown with a clear message.

Suggested cache location: `~/.claude-voice-models/`

### Stop Mechanism

Two ways to stop the voice loop:
1. **Voice**: say the configured stop phrase (default: "stop listening"). Checked in the LISTENING state after each transcription is finalized.
2. **Keyboard**: Ctrl+C sends SIGINT, caught by signal handler, triggers `stopVoiceLoop()`.

---

## Phases

### Phase 1: Project scaffolding ✅
- [x] Initialize npm project with TypeScript
- [x] Set up `package.json` with all dependencies (node-record-lpcm16, avr-vad, sherpa-onnx, kokoro-js, speaker, @anthropic-ai/claude-agent-sdk, @anthropic-ai/sdk)
- [x] Set up `tsconfig.json`
- [x] Create `.claude-plugin/plugin.json` manifest
- [x] Create `skills/voice/SKILL.md` with Bash invocation of sidecar
- [x] Create `sidecar/types.ts` with all shared interfaces

### Phase 2: Audio capture + VAD ✅
- [x] Implement `audio-capture.ts` (sox wrapper, 16kHz mono raw PCM stream, `bufferToFloat32` conversion)
- [x] Implement `vad.ts` (avr-vad initialization, async frame processing with Float32Array)
- [ ] Test: mic captures audio, VAD detects speech start/end
- [ ] Wire audio-capture --> VAD in a standalone test script

### Phase 3: Local STT ✅
- [x] Implement `stt.ts` (sherpa-onnx offline recognizer for Whisper, accumulate/transcribe pattern, Float32Array input)
- [ ] Test: feed recorded audio buffer, get batch transcription
- [ ] Wire audio-capture --> VAD --> STT: accumulate during speech, batch-transcribe on SPEECH_END, see transcript in console

### Phase 4: Endpointing ✅
- [x] Implement `endpointing.ts` (fast path: silence duration + word count, no punctuation reliance)
- [x] Add Haiku fallback for ambiguous cases (via @anthropic-ai/sdk)
- [x] Add timeout path
- [ ] Test: speak naturally, verify turn detection is accurate

### Phase 5: Claude session ✅
- [x] Implement `claude-session.ts` (Agent SDK V1 AsyncGenerator yielding SDKUserMessage objects, streaming event mapping via message.event.type path, permissionMode: "bypassPermissions", voice-mode systemPrompt, query.interrupt() support)
- [ ] Test: send text, receive streaming response events mapped to ClaudeStreamEvent

### Phase 6: Narration ✅
- [x] Implement `narration.ts` (event processing, sentence chunking, long_task template summaries)
- [ ] Test: feed mock ClaudeStreamEvents, verify correct text output

### Phase 7: Local TTS + speaker playback ✅
- [x] Implement `tts.ts` (kokoro-js init, Float32Array->Int16 PCM conversion, Speaker at 24kHz, TextSplitterStream for streaming, interrupt via Speaker destroy + queue clear)
- [ ] Test: text in, audio out through speakers
- [ ] Wire narration text output --> TTS --> speaker

### Phase 8: Main loop + interruption ✅
- [x] Implement `index.ts` (state machine with all transitions including PROCESSING->LISTENING fallback, module wiring, start/stop, signal handlers, stdout status printing, stop phrase detection, 300ms sustained-speech interruption filter)
- [ ] End-to-end test: full voice loop works

### Phase 9: Plugin packaging ✅
- [ ] Verify plugin loads in Claude Code via `claude --plugin-dir`
- [ ] Test `/voice` command triggers the sidecar via Bash
- [x] Add model download instructions to README.md
- [x] Document headphone recommendation

---

## Success Criteria

- `/voice` command starts the voice loop from within Claude Code
- User can speak naturally and see their speech transcribed
- Claude Code responds with full context (files, tools, conversation history)
- Claude's response is narrated aloud at sentence granularity
- Long-running tasks produce periodic spoken summaries
- User can interrupt Claude mid-speech by talking (with 300ms debounce)
- User can say "stop listening" to exit voice mode
- Entire STT/TTS/VAD pipeline runs locally with zero external API keys
- Latency from end-of-speech to first audio playback is under 2 seconds (excluding LLM time)
- Fail fast with clear error messages for: missing sox, missing STT model, TTS model download failure, mic permission denied, Claude session error
- Clean shutdown on SIGINT/SIGTERM (no leaked sox processes, no locked microphone)
- Status messages printed to stdout on state transitions
